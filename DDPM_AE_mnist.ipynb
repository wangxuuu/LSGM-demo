{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oem/anaconda3/envs/xu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from utils import *\n",
    "from models.diffusion import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from scipy.ndimage import rotate\n",
    "from itertools import chain\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random seed\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# cuda = True if torch.cuda.is_available() else False\n",
    "# FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "# torch.set_default_tensor_type(FloatTensor)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sample_dir = './results/MNIST_DDPM'\n",
    "model_dir = './results/checkpoints/DDPM_MNIST'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the same shape torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "joint_training = True\n",
    "load_available = True\n",
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "eps = 1e-6\n",
    "sigma_min = 0.001\n",
    "sigma_max = 10\n",
    "n_steps = 10\n",
    "annealed_step = 100\n",
    "\n",
    "num_steps = 100\n",
    "\n",
    "#制定每一步的beta\n",
    "betas = torch.linspace(-6,6,num_steps)\n",
    "betas = torch.sigmoid(betas)*(0.5e-2 - 1e-5)+1e-5\n",
    "\n",
    "#计算alpha、alpha_prod、alpha_prod_previous、alpha_bar_sqrt等变量的值\n",
    "alphas = 1-betas\n",
    "alphas_prod = torch.cumprod(alphas,0)\n",
    "alphas_prod_p = torch.cat([torch.tensor([1]).float(),alphas_prod[:-1]],0)\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "one_minus_alphas_bar_log = torch.log(1 - alphas_prod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)\n",
    "\n",
    "assert alphas.shape==alphas_prod.shape==alphas_prod_p.shape==\\\n",
    "alphas_bar_sqrt.shape==one_minus_alphas_bar_log.shape\\\n",
    "==one_minus_alphas_bar_sqrt.shape\n",
    "print(\"all the same shape\",betas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "dataset = torchvision.datasets.MNIST(root='../../Data/MNIST/',\n",
    "                                     train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(input_size=image_size, h_dim=h_dim, z_dim=z_dim, type='ce').to(device)\n",
    "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "# sn = SN_Model(device, n_steps, sigma_min, sigma_max, dim=z_dim, p = 0.3)\n",
    "sn = MLPDiffusion(num_steps)\n",
    "sn_optim = torch.optim.Adam(sn.parameters(), lr = 1e-3)\n",
    "joint_optim = torch.optim.Adam(params=chain(vae.parameters(), sn.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oem/anaconda3/envs/xu/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100], Step [100/469], Reconst Loss: 21895.4082, KL Div: 1386.0198, Diffuse loss: 0.9691\n",
      "Epoch[1/100], Step [200/469], Reconst Loss: 17678.4805, KL Div: 1897.8455, Diffuse loss: 1.0043\n",
      "Epoch[1/100], Step [300/469], Reconst Loss: 16089.0830, KL Div: 2026.5127, Diffuse loss: 0.9869\n",
      "Epoch[1/100], Step [400/469], Reconst Loss: 15024.9023, KL Div: 2522.7236, Diffuse loss: 1.0276\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 64\u001b[0m\n\u001b[1;32m     57\u001b[0m save_image(x_concat, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(sample_dir, \u001b[39m'\u001b[39m\u001b[39mreconst-\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m     59\u001b[0m \u001b[39m# Save the diffused ima ges\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m# dynamic = AnnealedLangevinDynamic(sigma_min, sigma_max, n_steps, annealed_step, sn, device, eps=eps)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# z_ = forward_proc(z, sigma_min, sigma_max, n_steps, device=device, only_final=True)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m# sample = dynamic.sampling(x.shape[0], z_dim, sample=z_, only_final=True)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m sample \u001b[39m=\u001b[39m ddim_sample(sn, num_steps\u001b[39m=\u001b[39;49mnum_steps, batch_size\u001b[39m=\u001b[39;49mx\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], dim\u001b[39m=\u001b[39;49mz_dim)\n\u001b[1;32m     65\u001b[0m \u001b[39m# sample = dynamic.sampling(x.shape[0], z_dim, only_final=True)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m out \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39mdecode(sample)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/wangxu/LSGM-demo/models/diffusion.py:146\u001b[0m, in \u001b[0;36mddim_sample\u001b[0;34m(model, num_steps, batch_size, dim, x_T)\u001b[0m\n\u001b[1;32m    144\u001b[0m     ts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([t] \u001b[39m*\u001b[39m x_T\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    145\u001b[0m     \u001b[39m# alphas = alphas_for_ts(ts)\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     x_t \u001b[39m=\u001b[39m ddim_previous(x_t, ts, model(x_t, torch\u001b[39m.\u001b[39;49mtensor(ts)))\n\u001b[1;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m x_t\n",
      "File \u001b[0;32m~/anaconda3/envs/xu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/wangxu/LSGM-demo/models/diffusion.py:37\u001b[0m, in \u001b[0;36mMLPDiffusion.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x,t):\n\u001b[1;32m     35\u001b[0m \u001b[39m#         x = x_0\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         \u001b[39mfor\u001b[39;00m idx,embedding_layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_embeddings):\n\u001b[0;32m---> 37\u001b[0m             t_embedding \u001b[39m=\u001b[39m embedding_layer(t)\n\u001b[1;32m     38\u001b[0m             x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinears[\u001b[39m2\u001b[39m\u001b[39m*\u001b[39midx](x)\n\u001b[1;32m     39\u001b[0m             x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m t_embedding\n",
      "File \u001b[0;32m~/anaconda3/envs/xu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xu/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/xu/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        if joint_training:\n",
    "            mu, log_var = vae.encode(x)\n",
    "            z = vae.reparameterize(mu, log_var)\n",
    "            x_reconst = vae.decode(z)\n",
    "            # Compute reconstruction loss and kl divergence\n",
    "            reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "            kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            loss_sn = diffusion_loss_fn(sn, z,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,num_steps)\n",
    "            loss = loss_sn + reconst_loss + kl_div\n",
    "\n",
    "            joint_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            joint_optim.step()\n",
    "        else:\n",
    "            #============= First Stage: Update VAE ==============#\n",
    "            # Forward pass\n",
    "            x_reconst, mu, log_var = vae(x)\n",
    "            # Compute reconstruction loss and kl divergence\n",
    "            # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
    "            reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "            kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            \n",
    "            # Backprop and optimize\n",
    "            vae_loss = reconst_loss + kl_div\n",
    "            vae_optimizer.zero_grad()\n",
    "            vae_loss.backward()\n",
    "            vae_optimizer.step()\n",
    "\n",
    "            #============= Second Stage: Update SN ==============#\n",
    "            mu, log_var = vae.encode(x)\n",
    "            z = vae.reparameterize(mu, log_var)\n",
    "\n",
    "            loss_sn = sn.loss(z)\n",
    "            vae_optimizer.zero_grad()\n",
    "            sn_optim.zero_grad()\n",
    "            loss_sn.backward()\n",
    "            sn_optim.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}, Diffuse loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item(), loss_sn.item()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Save the sampled images\n",
    "        z = torch.randn(x.shape[0], z_dim).to(device)\n",
    "        out = vae.decode(z).view(-1, 1, 28, 28)\n",
    "        x_concat = torch.cat([x.view(-1, 1, 28, 28), out], dim=3)\n",
    "        save_image(x_concat, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "\n",
    "        # Save the reconstructed images\n",
    "        out, _, _ = vae(x)\n",
    "        x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
    "        save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch+1)))\n",
    "\n",
    "        # Save the diffused ima ges\n",
    "        # dynamic = AnnealedLangevinDynamic(sigma_min, sigma_max, n_steps, annealed_step, sn, device, eps=eps)\n",
    "        # z_ = forward_proc(z, sigma_min, sigma_max, n_steps, device=device, only_final=True)\n",
    "        # sample = dynamic.sampling(x.shape[0], z_dim, sample=z_, only_final=True)\n",
    "\n",
    "        sample = ddim_sample(sn, num_steps=num_steps, batch_size=x.shape[0], dim=z_dim)\n",
    "        # sample = dynamic.sampling(x.shape[0], z_dim, only_final=True)\n",
    "        out = vae.decode(sample).view(-1, 1, 28, 28)\n",
    "        x_concat = torch.cat([x.view(-1, 1, 28, 28), out], dim=3)\n",
    "        save_image(x_concat, os.path.join(sample_dir, 'diffuse-{}.png'.format(epoch+1)))\n",
    "torch.save({'sn_state':sn.state_dict(), 'vae_state':vae.state_dict()}, model_dir+'ckpt.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('xu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca77da5a2651d428d298ea3d77cb6cfe29dc494bcc518222eca7638029259bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
